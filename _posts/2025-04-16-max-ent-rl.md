---
layout: post
category: unpublished
title: Soft Value Functions
description: Derivation of soft value functions used in SAC and various IRL papers
# image: /assets/reinforce_algo_sutton.png # URL to the post's featured image
---

By definition, the value of a state $$s$$ (under policy $$\pi$$) in an MDP is the expected discounted return under the policy starting from that state

$$
\begin{align*}
V_{\pi}(s) &= \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \vert s_0 = s \right] \\
Q_{\pi}(s, a) &= \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \vert s_0 = s, a_0 = a\right] \\\\
V_{\pi}(s) &= \sum_{a \sim \pi(a \vert s)} \pi(a \vert s) Q_{\pi}(s, a) = \mathbb{E}_{a \sim \pi} \left[ Q(s, a) \right]
\end{align*}
$$

Often, it is also beneficial to maximise the step-wise entropy of the policy (at each state). The entropy is defined by 

$$H(\pi(a \vert s)) = \mathbb{E}_{a,s \sim \rho_{\pi}} \left[ - \log \pi(a \vert s) \right]$$

Given this criteria, the definition of the return changes, causing the definition of the value functions to change. The entropy regularised value functions are called "soft" value functions.

$$
\begin{align*}
V_{\pi}^{\text{soft}}(s) &= \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) + \gamma^t \alpha H(\pi(. \vert s_t)) \vert s_0 = s \right] \\
Q_{\pi}^{\text{soft}}(s, a) &= \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) + \sum_{t=1}^{\infty} \gamma^t \alpha H(\pi(. \vert s_t)) \vert s_0 = s, a_0 = a \right] \\\\
V_{\pi}^{\text{soft}}(s) &= \mathbb{E}_{a \sim \pi} \left[ Q_{\pi}^{\text{soft}}(s, a) \right] + \alpha H(\pi(. \vert s))
\end{align*}
$$

In the soft action value definition, we only consider the entropy contribuion from $$s'$$ onwards since the action $$a$$ is already given to us. So no matter what the policy says, its entropy contribution should be zero. In the final expression, $$\alpha H(\pi(. \vert s))$$ is hence the added contribution of entropy at state $$s$$ that comes from the state value function (since the entropy contribution at state $$s$$ from the action value function is zero). From $$s'$$ onwards, the entropy contribution of the action value is already accounted for in the state value. 

Finally, we can also compare the traditional Bellman equation to the Soft Bellman Equation.

$$
\begin{align*}
V_{\pi}(s) &= \sum_{a} \pi(a \vert s) \sum_{s'} P(s' \vert s, a) \left[ R(s, s, s') + \gamma \underbrace{V_{\pi}(s')}_{\text{expanded as Q below}} \right] \\
Q_{\pi}(s, a) &= \sum_{s'} P(s' \vert s, a) \left[ R(s, a, s') + \gamma V_{\pi}(s') \right] \\
Q_{\pi}(s, a) &= \mathbb{E}_{s' \sim P(s' \vert s,a)} \left[ R(s, a, s') + \gamma \sum_{a'} \pi(a' \vert s') Q_{\pi}(s', a') \right]
\end{align*}
$$

In the soft case:

$$
\begin{align*}
Q_{\pi}^{\text{soft}}(s, a) &= \mathbb{E}_{s' \sim P(s' \vert s,a)} \left[ R(s, a, s') + \gamma \underbrace{V_{\pi}^{\text{soft}}(s')}_{\text{expanded below}} \right] \\
Q_{\pi}^{\text{soft}}(s, a) &= \mathbb{E}_{s' \sim P(s' \vert s,a)} \left[ R(s, a, s') + \gamma \: \mathbb{E}_{a' \sim \pi} \left[ Q_{\pi}^{\text{soft}}(s', a') \right] + \alpha H(\pi(. \vert s')) \right]
\end{align*}
$$